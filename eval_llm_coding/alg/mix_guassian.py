# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/01_mix_guassian_copilot.ipynb.

# %% auto 0
__all__ = ['MixtureOfGaussians']

# %% ../../nbs/01_mix_guassian_copilot.ipynb 3
import numpy as np

# %% ../../nbs/01_mix_guassian_copilot.ipynb 4
import torch
import torch.nn as nn
import torch.distributions as dist

class MixtureOfGaussians(nn.Module):
    def __init__(self, n_components, n_features, low=-4.0, high=4.0):
        super(MixtureOfGaussians, self).__init__()
        self.n_components = n_components
        self.n_features = n_features
        self.means = nn.Parameter(torch.empty(n_components, n_features).uniform_(low, high))
        self.cov_factors = nn.Parameter(torch.stack([torch.tril(torch.eye(n_features)) for _ in range(self.n_components)]))
        self.log_weights = nn.Parameter(torch.zeros(n_components))

    @property
    def covariances(self):
        return torch.matmul(self.cov_factors, self.cov_factors.transpose(1, 2))

    @property
    def weights(self):
        return self.log_weights.softmax(dim=0)
    
    def forward(self, x):
        gaussians = [dist.MultivariateNormal(self.means[i], scale_tril=self.cov_factors[i]) \
                     for i in range(self.n_components)]
        log_probs = torch.stack([gaussian.log_prob(x) for gaussian in gaussians], dim=-1)
        log_probs = log_probs + self.log_weights.log_softmax(dim=0)
        return -torch.logsumexp(log_probs, dim=-1).mean()

    def _e_step(self, X):
        """Compute responsibilities (E-step)."""
        n_samples = X.shape[0]
        weights = self.log_weights.softmax(dim=0)  # Normalize weights
        responsibilities = torch.zeros(n_samples, self.n_components)
        
        for k in range(self.n_components):
            # Ensure positive-definiteness of covariance matrices
            mvn = dist.MultivariateNormal(self.means[k], scale_tril=self.cov_factors[k])
            responsibilities[:, k] = weights[k] * mvn.log_prob(X).exp()
            # print(f"K {k}: mean {self.means[k]} responsibilities{responsibilities[-10:, k]}")
        
        # Normalize responsibilities across components
        responsibilities /= responsibilities.sum(dim=1, keepdim=True)
        # print(f"responsibilities: {responsibilities[:10, :]}")
        return responsibilities

    def predict(self, X):
        responsibilities = self._e_step(X)
        return responsibilities.argmax(dim=1)

    def sample(self, num_samples):
        component = torch.multinomial(self.log_weights.softmax(dim=0), num_samples, replacement=True)
        samples = torch.stack([dist.MultivariateNormal(self.means[i], scale_tril=self.cov_factors[i]).sample((num_samples,)) for i in range(self.n_components)])
        return samples[component, torch.arange(num_samples)]
