# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/00_em_openai.ipynb.

# %% auto 0
__all__ = ['EMGMM']

# %% ../../nbs/00_em_openai.ipynb 3
import numpy as np
from scipy.stats import multivariate_normal


# %% ../../nbs/00_em_openai.ipynb 4
class EMGMM:
    def __init__(self, n_components, n_features, low_value=-4.0, high_value=4.0, max_iter=100, tol=1e-6):
        self.n_components = n_components
        self.n_features = n_features
        self.max_iter = max_iter
        self.tol = tol

        # Initialize parameters
        self.weights = np.ones(self.n_components) / self.n_components
        self.means = np.random.uniform(low=low_value, high=high_value, size=(self.n_components, n_features))
        self.covariances = np.array([np.eye(n_features) for _ in range(self.n_components)])


    def fit(self, X):
        log_likelihood_old = 0

        for iteration in range(self.max_iter):
            print(f"Iteration: {iteration + 1}/{self.max_iter} weights:{self.weights} means:{self.means}, nll: {-log_likelihood_old}")
            # E-step
            responsibilities = self._e_step(X)
            
            # M-step
            self._m_step(X, responsibilities)

            # Compute log-likelihood
            log_likelihood = self._log_likelihood(X)
            if abs(log_likelihood - log_likelihood_old) < self.tol:
                break
            log_likelihood_old = log_likelihood
    
    def _e_step(self, X):
        weighted_pdfs = np.zeros((X.shape[0], self.n_components))
        for k in range(self.n_components):
            weighted_pdfs[:, k] = self.weights[k] * multivariate_normal.pdf(
                X, mean=self.means[k], cov=self.covariances[k]
            )
        responsibilities = weighted_pdfs / weighted_pdfs.sum(axis=1, keepdims=True)
        return responsibilities

    def _m_step(self, X, responsibilities):
        N_k = responsibilities.sum(axis=0)
        self.weights = N_k / X.shape[0]
        self.means = (responsibilities.T @ X) / N_k[:, None]
        for k in range(self.n_components):
            diff = X - self.means[k]
            self.covariances[k] = (responsibilities[:, k][:, None] * diff).T @ diff / N_k[k]

    def _log_likelihood(self, X):
        log_likelihood = 0
        for k in range(self.n_components):
            log_likelihood += self.weights[k] * multivariate_normal.pdf(
                X, mean=self.means[k], cov=self.covariances[k]
            )
        return np.sum(np.log(log_likelihood))
    
    def predict(self, X):
        responsibilities = self._e_step(X)
        return np.argmax(responsibilities, axis=1)

